"""
Tester Agent (Judge) - Ex√©cute les tests et analyse les r√©sultats
==================================================================
Le Judge ex√©cute pytest et d√©cide si on continue ou si c'est termin√©.
"""

import json
from src.utils.logger import log_experiment, ActionType
from src.utils.analysis_tools import run_pytest
from src.utils.gemini_client import call_gemini_json
from src.config import get_model_name


def load_prompt():
    """Charge le prompt syst√®me du testeur."""
    with open("src/prompts/tester_prompt.txt", "r", encoding="utf-8") as file:
        return file.read()


def run_tester_agent(target_dir: str, model_used: str = None) -> dict:
    """
    Ex√©cute l'agent Testeur en utilisant les outils du Toolsmith.
    
    Args:
        target_dir (str): R√©pertoire √† tester
        model_used (str): Mod√®le LLM utilis√© (None = utilise config.py)
    
    Returns:
        dict: R√©sultat avec 'test_status', 'failing_tests', 'action', 'should_continue'
    """
    
    # Utiliser le mod√®le par d√©faut si non sp√©cifi√©
    if model_used is None:
        model_used = get_model_name()
    
    system_prompt = load_prompt()
    
    print(f"üß™ [JUDGE] Ex√©cution des tests dans {target_dir}...")
    
    # ‚úÖ UTILISER L'OUTIL DU TOOLSMITH pour ex√©cuter pytest
    pytest_results = run_pytest(target_dir)
    
    # Analyser les r√©sultats
    failing_tests = []
    total_tests = 0
    failed_tests = 0
    
    for result in pytest_results:
        if not result.get("path"):  # Skip empty entries
            continue
        
        total_tests += 1
        
        if result.get("test_error"):
            failed_tests += 1
            failing_tests.append({
                "test_file": result["path"],
                "error_type": "TestFailure",
                "error_message": result.get("remarks", "Test failed"),
                "return_code": result.get("code", 1)
            })
    
    # D√©terminer le statut initial
    if failed_tests == 0 and total_tests > 0:
        initial_status = "success"
    elif total_tests == 0:
        initial_status = "no_tests"
    else:
        initial_status = "failure"
    
    # Construire le prompt pour le LLM
    pytest_summary = json.dumps(pytest_results, indent=2, ensure_ascii=False)
    
    input_prompt = f"""{system_prompt}

=== R√âSULTATS D'EX√âCUTION PYTEST ===
R√©pertoire test√©: {target_dir}
Tests trouv√©s: {total_tests}
Tests √©chou√©s: {failed_tests}

D√©tails complets:
{pytest_summary}

=== MISSION ===
Analysez ces r√©sultats et r√©pondez UNIQUEMENT en JSON:

{{
  "test_status": "success" | "failure" | "no_tests",
  "action": "validate" | "return_to_corrector",
  "analysis": "Votre analyse factuelle des probl√®mes",
  "failing_tests": [
    {{
      "test_name": "nom du test qui √©choue",
      "error_type": "type d'erreur",
      "error_message": "message r√©sum√©"
    }}
  ]
}}

R√àGLES:
- Si TOUS les tests passent ‚Üí test_status="success", action="validate"
- Si AU MOINS un test √©choue ‚Üí test_status="failure", action="return_to_corrector"
- Si aucun test trouv√© ‚Üí test_status="no_tests", action="return_to_corrector"
"""
    
    # ‚úÖ APPEL √Ä L'API GEMINI
    try:
        output_response_json = call_gemini_json(input_prompt, model_name=model_used)
        output_response = json.dumps(output_response_json, indent=2, ensure_ascii=False)
        
        # Extraire les informations importantes
        test_status = output_response_json.get("test_status", initial_status)
        action = output_response_json.get("action", "return_to_corrector")
        analysis = output_response_json.get("analysis", "")
        llm_failing_tests = output_response_json.get("failing_tests", [])
        
        # Utiliser les tests d√©faillants du LLM s'ils sont fournis, sinon ceux qu'on a d√©tect√©s
        final_failing_tests = llm_failing_tests if llm_failing_tests else failing_tests
        
        # üìã LOGGING OBLIGATOIRE
        log_experiment(
            agent_name="Judge",
            model_used=model_used,
            action=ActionType.ANALYSIS,
            details={
                "target_dir": target_dir,
                "input_prompt": input_prompt,
                "output_response": output_response,
                "test_status": test_status,
                "total_tests": total_tests,
                "failed_tests": failed_tests,
                "pytest_tool_results": pytest_results
            },
            status="SUCCESS"
        )
        
        # Afficher le r√©sultat
        if test_status == "success":
            print("‚úÖ [JUDGE] Tous les tests passent!")
        elif test_status == "no_tests":
            print("‚ö†Ô∏è [JUDGE] Aucun test trouv√©")
        else:
            print(f"‚ùå [JUDGE] {len(final_failing_tests)} test(s) √©choue(nt)")
            if analysis:
                print(f"   Analyse: {analysis[:100]}...")
        
        return {
            "test_status": test_status,
            "failing_tests": final_failing_tests,
            "action": action,
            "should_continue": (action == "return_to_corrector"),
            "summary": f"{failed_tests}/{total_tests} tests failed" if failed_tests > 0 else "All tests passed",
            "analysis": analysis
        }
        
    except Exception as e:
        error_msg = f"Erreur lors de l'appel √† Gemini: {str(e)}"
        
        # En cas d'erreur, utiliser les r√©sultats bruts
        log_experiment(
            agent_name="Judge",
            model_used=model_used,
            action=ActionType.DEBUG,
            details={
                "target_dir": target_dir,
                "input_prompt": input_prompt,
                "output_response": error_msg,
                "error": str(e)
            },
            status="FAILURE"
        )
        
        # Retourner quand m√™me un r√©sultat utilisable
        return {
            "test_status": initial_status,
            "failing_tests": failing_tests,
            "action": "validate" if initial_status == "success" else "return_to_corrector",
            "should_continue": (initial_status != "success"),
            "error": error_msg
        }


if __name__ == "__main__":
    # Test avec un sandbox d'exemple
    test_dir = "./sandbox/example"
    result = run_tester_agent(test_dir)
    
    print("\n=== R√©sultat Final du Judge ===")
    print(json.dumps(result, indent=2, ensure_ascii=False))