# src/agents/tester_agent.py

import os
import json
import subprocess
from pathlib import Path
from src.utils.logger import log_experiment, ActionType

def load_prompt():
    """Charge le prompt systÃ¨me du testeur."""
    with open("src/prompts/tester_prompt.txt", "r", encoding="utf-8") as file:
        return file.read()


def run_pytest(target_dir: str) -> str:
    """
    ExÃ©cute pytest sur le rÃ©pertoire cible et capture les rÃ©sultats.
    
    Args:
        target_dir (str): Chemin du dossier Ã  tester.
    
    Returns:
        str: Sortie brute de pytest (stdout + stderr).
    """
    try:
        result = subprocess.run(
            ["pytest", target_dir, "-v", "--tb=short"],
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return "TIMEOUT: pytest execution exceeded 30 seconds"
    except FileNotFoundError:
        return "ERROR: pytest not installed. Run: pip install pytest"


def run_tester_agent(pytest_logs: str, target_dir: str, model_used: str = "gemini-2.5-flash") -> dict:
    """
    ExÃ©cute l'agent Testeur pour analyser les logs de pytest.
    
    Args:
        pytest_logs (str): Sortie brute de pytest.
        target_dir (str): RÃ©pertoire testÃ© (pour contexte).
        model_used (str): ModÃ¨le LLM utilisÃ©.
    
    Returns:
        dict: RÃ©sultat structurÃ© avec 'test_status', 'failing_tests', 'action'.
    """
    
    system_prompt = load_prompt()
    
    # Construire le prompt complet
    # NOTE: Logs complets car essentiels pour le diagnostic
    input_prompt = f"""{system_prompt}

=== LOGS D'EXÃ‰CUTION PYTEST ===
{pytest_logs[:2000]}{"..." if len(pytest_logs) > 2000 else ""}

=== CONTEXTE ===
RÃ©pertoire testÃ©: {target_dir}

Analysez ces logs et rÃ©pondez UNIQUEMENT en JSON.
"""
    
    # âš ï¸ INTÃ‰GRATION MODÃˆLE IA (Ã  complÃ©ter selon votre orchestrateur)
    # Pour l'instant, simulation basique
    # Ã€ remplacer par: output_response = call_gemini_api(input_prompt)
    
    # DÃ©tection simple d'erreurs pour la simulation
    if "FAILED" in pytest_logs or "ERROR" in pytest_logs:
        test_status = "failure"
        failing_tests = [
            {
                "test_name": "test_example",
                "error_type": "AssertionError",
                "error_message": "Extracted from pytest logs"
            }
        ]
        action = "return_to_corrector"
    else:
        test_status = "success"
        failing_tests = []
        action = "validate"
    
    output_response = json.dumps({
        "test_status": test_status,
        "failing_tests": failing_tests,
        "action": action
    })
    
    # ğŸ“‹ LOGGING OBLIGATOIRE
    log_experiment(
        agent_name="Tester_Agent",
        model_used=model_used,
        action=ActionType.DEBUG,
        details={
            "target_dir": target_dir,
            "input_prompt": input_prompt,  # âœ… OBLIGATOIRE
            "output_response": output_response,  # âœ… OBLIGATOIRE
            "pytest_output_length": len(pytest_logs),
            "test_status": test_status
        },
        status="SUCCESS"
    )
    
    # Traiter la rÃ©ponse
    try:
        result = json.loads(output_response)
        return {
            "test_status": result.get("test_status", "unknown"),
            "failing_tests": result.get("failing_tests", []),
            "action": result.get("action", "unknown"),
            "should_continue": result.get("action") == "return_to_corrector"
        }
    except json.JSONDecodeError as e:
        log_experiment(
            agent_name="Tester_Agent",
            model_used=model_used,
            action=ActionType.DEBUG,
            details={
                "target_dir": target_dir,
                "input_prompt": input_prompt,
                "output_response": output_response,
                "error": str(e)
            },
            status="FAILURE"
        )
        return {
            "test_status": "error",
            "action": "error",
            "error": f"Invalid JSON response: {str(e)}"
        }


def validate_and_test(target_dir: str, model_used: str = "gemini-2.5-flash") -> dict:
    """
    Pipeline complet : run pytest â†’ analyze with Tester Agent.
    
    Args:
        target_dir (str): RÃ©pertoire Ã  tester.
        model_used (str): ModÃ¨le LLM utilisÃ©.
    
    Returns:
        dict: RÃ©sultat final du test.
    """
    
    print(f"ğŸ§ª Running pytest on {target_dir}...")
    pytest_output = run_pytest(target_dir)
    
    print("ğŸ¤– Analyzing test results with Tester Agent...")
    result = run_tester_agent(
        pytest_logs=pytest_output,
        target_dir=target_dir,
        model_used=model_used
    )
    
    return result


if __name__ == "__main__":
    # Test local
    test_dir = "./sandbox/example"
    
    # CrÃ©er un dossier de test minimal pour dÃ©mo
    os.makedirs(test_dir, exist_ok=True)
    
    # Ã‰crire un test simple
    test_file = Path(test_dir) / "test_example.py"
    test_file.write_text("""
def test_simple():
    assert 1 + 1 == 2

def test_failure():
    assert 1 + 1 == 3
""")
    
    result = validate_and_test(test_dir)
    
    print("=== RÃ©sultat du Testeur ===")
    print(json.dumps(result, indent=2, ensure_ascii=False))
